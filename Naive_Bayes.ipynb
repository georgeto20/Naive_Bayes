{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "Naive Bayes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/georgeto20/Naive_Bayes/blob/master/Naive_Bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP2yMg67i2Py",
        "colab_type": "code",
        "outputId": "34abefbc-8efe-45b6-f251-1916cc11794e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEkPWpJci2Qo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_vocabulary(D):\n",
        "    \"\"\"\n",
        "    Given a list of documents, where each document is represented as\n",
        "    a list of tokens, return the resulting vocabulary. The vocabulary\n",
        "    should be a set of tokens which appear more than once in the entire\n",
        "    document collection plus the \"<unk>\" token.\n",
        "    \"\"\"\n",
        "    words = {}\n",
        "    for document in D:\n",
        "        for token in document:\n",
        "            if token not in words.keys():\n",
        "                words[token] = 1\n",
        "            else:\n",
        "                words[token] += 1\n",
        "    vocab = [token for token in words.keys() if words[token] > 1]\n",
        "    vocab.append(\"<unk>\")\n",
        "    return set(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJXHDtQri2Q0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BBoWFeaturizer(object):\n",
        "    def convert_document_to_feature_dictionary(self, doc, vocab):\n",
        "        \"\"\"\n",
        "        Given a document represented as a list of tokens and the vocabulary\n",
        "        as a set of tokens, compute the binary bag-of-words feature representation.\n",
        "        This function should return a dictionary which maps from the name of the\n",
        "        feature to the value of that feature.\n",
        "        \"\"\"\n",
        "        bbow = {}\n",
        "        for token in doc:\n",
        "            if token in vocab:\n",
        "                bbow[token] = 1\n",
        "            else:\n",
        "                bbow[\"<unk>\"] = 1\n",
        "        return bbow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiHxDWWTi2RB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CBoWFeaturizer(object):\n",
        "    def convert_document_to_feature_dictionary(self, doc, vocab):\n",
        "        \"\"\"\n",
        "        Given a document represented as a list of tokens and the vocabulary\n",
        "        as a set of tokens, compute the count bag-of-words feature representation.\n",
        "        This function should return a dictionary which maps from the name of the\n",
        "        feature to the value of that feature.\n",
        "        \"\"\"\n",
        "        cbow = {}\n",
        "        for token in doc:\n",
        "            if token in vocab:\n",
        "                if token not in cbow.keys():\n",
        "                    cbow[token] = 1\n",
        "                else:\n",
        "                    cbow[token] += 1\n",
        "            else:\n",
        "                if \"<unk>\" not in cbow.keys():\n",
        "                    cbow[\"<unk>\"] = 1\n",
        "                else:\n",
        "                    cbow[\"<unk>\"] += 1\n",
        "        return cbow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF9o4UAxi2RK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_idf(D, vocab):\n",
        "    \"\"\"\n",
        "    Given a list of documents D and the vocabulary as a set of tokens,\n",
        "    where each document is represented as a list of tokens, return the IDF scores\n",
        "    for every token in the vocab. The IDFs should be represented as a dictionary that\n",
        "    maps from the token to the IDF value. If a token is not present in the\n",
        "    vocab, it should be mapped to \"<unk>\".\n",
        "    \"\"\"\n",
        "    idfs = {}\n",
        "    for doc in D:\n",
        "        unk_doc = False\n",
        "        true_doc = [token for content in doc for token in content]\n",
        "        for token in set(true_doc):\n",
        "            if token in vocab:\n",
        "                if token not in idfs.keys():\n",
        "                    idfs[token] = 1\n",
        "                else:\n",
        "                    idfs[token] += 1\n",
        "            elif not unk_doc:\n",
        "                if \"<unk>\" not in idfs.keys():\n",
        "                    idfs[\"<unk>\"] = 1\n",
        "                else:\n",
        "                    idfs[\"<unk>\"] += 1\n",
        "                unk_doc = True\n",
        "    idfs = dict(zip(idfs.keys(), np.log([len(D) / value for value in idfs.values()])))\n",
        "    return idfs\n",
        "    \n",
        "class TFIDFFeaturizer(object):\n",
        "    def __init__(self, idf):\n",
        "        \"\"\"The idf scores computed via `compute_idf`.\"\"\"\n",
        "        self.idf = idf\n",
        "    \n",
        "    def convert_document_to_feature_dictionary(self, doc, vocab):\n",
        "        \"\"\"\n",
        "        Given a document represented as a list of tokens and\n",
        "        the vocabulary as a set of tokens, compute\n",
        "        the TF-IDF feature representation. This function\n",
        "        should return a dictionary which maps from the name of the\n",
        "        feature to the value of that feature.\n",
        "        \"\"\"\n",
        "        tfidf = {}\n",
        "        for token in doc:\n",
        "            if token in vocab:\n",
        "                if token not in tfidf.keys():\n",
        "                    tfidf[token] = 1\n",
        "                else:\n",
        "                    tfidf[token] += 1\n",
        "            else:\n",
        "                if \"<unk>\" not in tfidf.keys():\n",
        "                    tfidf[\"<unk>\"] = 1\n",
        "                else:\n",
        "                    tfidf[\"<unk>\"] += 1\n",
        "        for key in tfidf.keys():\n",
        "            tfidf[key] *= self.idf[key]\n",
        "        return tfidf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Imvp6eoki2RO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "from collections import Counter\n",
        "\n",
        "def load_dataset(file_path, counts=False, hashtag=False, at=False):\n",
        "    D = []\n",
        "    y = []\n",
        "    csv = pd.read_csv(file_path)\n",
        "    for index, row in csv.iterrows():\n",
        "        main_content = []\n",
        "        if type(row['text']) == str:\n",
        "            main_content = row['text'].split()\n",
        "        hashtag_count = \"\"\n",
        "        at_count = \"\"\n",
        "        if counts:\n",
        "            hashtag_count = str(row['hashcount'])\n",
        "            at_count = str(row['atcount'])\n",
        "        hashtag_content = []\n",
        "        if hashtag:\n",
        "            if type(row['hashcontent']) == str:\n",
        "                hashes = ast.literal_eval(row['hashcontent'])\n",
        "                hashes = [ht.strip() for ht in hashes]\n",
        "                hashtag_content = hashes\n",
        "        at_content = []\n",
        "        if at:\n",
        "            if type(row['atcontent']) == str:\n",
        "                true_at = \"\".join(c for c in row['atcontent'] if c not in ('.',','))\n",
        "                at_content = true_at.split()\n",
        "        all_content = [main_content, hashtag_count, hashtag_content, at_count, at_content]\n",
        "        D.append(all_content)\n",
        "        y.append(row['label'])\n",
        "    return D, y\n",
        "\n",
        "def convert_to_features(D, featurizer, vocab, hashtag_vocab, at_vocab):\n",
        "    X = []\n",
        "    for doc in D:\n",
        "        dictionary_to_append = Counter(featurizer.convert_document_to_feature_dictionary(doc[0], vocab))\n",
        "        if len(doc[1]) > 0:\n",
        "            dictionary_to_append += Counter({doc[1]: 1})\n",
        "        if len(hashtag_vocab) > 1:\n",
        "            dictionary_to_append += Counter(featurizer.convert_document_to_feature_dictionary(doc[2], hashtag_vocab))\n",
        "        if len(doc[3]) > 0:\n",
        "            dictionary_to_append += Counter({doc[3]: 1})\n",
        "        if len(at_vocab) > 1:\n",
        "            dictionary_to_append += Counter(featurizer.convert_document_to_feature_dictionary(doc[4], at_vocab))\n",
        "        X.append(dict(dictionary_to_append))\n",
        "    return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gL8YS4Yqi2RR",
        "colab_type": "code",
        "outputId": "4ea9a023-7a33-4093-c846-f836df1ff7b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from collections import Counter\n",
        "a = Counter({1: 2, 2: 3})\n",
        "b = Counter({1: 3, 3: 4})\n",
        "dict(a + b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 5, 2: 3, 3: 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNPc31VKi2RX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_naive_bayes(X, y, k, vocab):\n",
        "    \"\"\"\n",
        "    Computes the statistics for the Naive Bayes classifier.\n",
        "    X is a list of feature representations, where each representation\n",
        "    is a dictionary that maps from the feature name to the value.\n",
        "    y is a list of integers that represent the labels.\n",
        "    k is a float which is the smoothing parameters.\n",
        "    vocab is the set of vocabulary tokens.\n",
        "    \n",
        "    Returns two values:\n",
        "        p_y: A dictionary from the label to the corresponding p(y) score\n",
        "        p_v_y: A nested dictionary where the outer dictionary's key is\n",
        "            the label and the innner dictionary maps from a feature\n",
        "            to the probability p(v|y). For example, `p_v_y[1][\"hello\"]`\n",
        "            should be p(v=\"hello\"|y=1).\n",
        "    \"\"\"\n",
        "    unique_y = set(y)\n",
        "    p_y = {label: 0 for label in unique_y}\n",
        "    for label in y:\n",
        "        p_y[label] += 1/len(y)\n",
        "    numerator = {label: {} for label in unique_y}\n",
        "    denominator = {label: 0 for label in unique_y}\n",
        "    for i, feat_rep in enumerate(X):\n",
        "        label = y[i]\n",
        "        for token in feat_rep:\n",
        "            if token not in numerator[label].keys():\n",
        "                numerator[label][token] = feat_rep[token]\n",
        "            else:\n",
        "                numerator[label][token] += feat_rep[token]\n",
        "            denominator[label] += feat_rep[token]\n",
        "    p_v_y = {label: {} for label in unique_y}\n",
        "    for label, feat_rep in numerator.items():\n",
        "        for token in vocab:\n",
        "            if token in feat_rep:\n",
        "                p_v_y[label][token] = (k + feat_rep[token]) / (denominator[label] + k*len(vocab))\n",
        "            else:\n",
        "                p_v_y[label][token] = k / (denominator[label] + k*len(vocab))\n",
        "    return p_y, p_v_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSVBGW_Wi2Rb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_naive_bayes(D, p_y, p_v_y):\n",
        "    \"\"\"\n",
        "    Runs the prediction rule for Naive Bayes. D is a list of documents,\n",
        "    where each document is a list of tokens.\n",
        "    p_y and p_v_y are output from `train_naive_bayes`.\n",
        "    \n",
        "    Note that any token which is not in p_v_y should be mapped to\n",
        "    \"<unk>\". Further, the input dictionaries are probabilities. You\n",
        "    should convert them to log-probabilities while you compute\n",
        "    the Naive Bayes prediction rule to prevent underflow errors.\n",
        "    \n",
        "    Returns two values:\n",
        "        predictions: A list of integer labels, one for each document,\n",
        "            that is the predicted label for each instance.\n",
        "        confidences: A list of floats, one for each document, that is\n",
        "            p(y|d) for the corresponding label that is returned.\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    confidences = []\n",
        "    keys_list = list(p_y.keys())\n",
        "    for doc in D:\n",
        "        token_sum = [0 for label in p_y.keys()]\n",
        "        token_product = [[] for label in p_y.keys()]\n",
        "        true_doc = [token for content in doc for token in content]\n",
        "        for token in true_doc:\n",
        "            for label in p_y.keys():\n",
        "                if token in p_v_y[label]:\n",
        "                    token_sum[keys_list.index(label)] += np.log(p_v_y[label][token])\n",
        "                    token_product[keys_list.index(label)].append(p_v_y[label][token])\n",
        "                else:\n",
        "                    token_sum[keys_list.index(label)] += np.log(p_v_y[label][\"<unk>\"])\n",
        "                    token_product[keys_list.index(label)].append(p_v_y[label][\"<unk>\"])\n",
        "        best_label = keys_list[0]\n",
        "        for label in keys_list[1:]:\n",
        "            if token_sum[keys_list.index(label)] + np.log(p_y[label]) > token_sum[keys_list.index(best_label)] + np.log(p_y[best_label]):\n",
        "                best_label = label\n",
        "        predictions.append(best_label)\n",
        "        p_d = np.sum([np.prod(token_product[keys_list.index(label)]) * p_y[label] for label in p_y.keys()])\n",
        "        if p_d == 0:\n",
        "            confidences.append(1)\n",
        "        else:\n",
        "            confidences.append(np.prod(token_product[keys_list.index(best_label)]) * p_y[best_label] / p_d)\n",
        "    return predictions, confidences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "_kscjQJRi2Ri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FIRST EXPERIMENT\n",
        "D_train, y_train = load_dataset('/content/gdrive/Shared drives/CIS519/data/new data/train_20_now.csv', True, True, True)\n",
        "D_valid, y_valid = load_dataset('/content/gdrive/Shared drives/CIS519/data/new data/validation_20_now.csv', True, True, True)\n",
        "D_test, y_test = load_dataset('/content/gdrive/Shared drives/CIS519/data/new data/test_20_now.csv', True, True, True)\n",
        "\n",
        "vocab = get_vocabulary([doc[0] for doc in D_train])\n",
        "hashtag_vocab = get_vocabulary([doc[2] for doc in D_train])\n",
        "at_vocab = get_vocabulary([doc[4] for doc in D_train])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITPBMOAHi2Rn",
        "colab_type": "code",
        "outputId": "717d8c74-f782-49b7-9904-2d6aeb20fbd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "print(\"BBoW:\")\n",
        "featurizer = BBoWFeaturizer()\n",
        "X_train = convert_to_features(D_train, featurizer, vocab, hashtag_vocab, at_vocab)\n",
        "best_k = 0\n",
        "best_accuracy = 0\n",
        "best_p_y = {}\n",
        "best_p_v_y = {}\n",
        "for k in [0.001, 0.01, 0.1, 1.0, 10.0]:\n",
        "    p_y, p_v_y = train_naive_bayes(X_train, y_train, k, vocab)\n",
        "    predictions, confidences = predict_naive_bayes(D_valid, p_y, p_v_y)\n",
        "    accuracy = len([i for i in range(len(predictions)) if predictions[i] == y_valid[i]]) / len(y_valid)\n",
        "    print(\"k: %g, accuracy: %.4f\" %(k, accuracy))\n",
        "    if accuracy > best_accuracy:\n",
        "        best_k = k\n",
        "        best_accuracy = accuracy\n",
        "        best_p_y = p_y\n",
        "        best_p_v_y = p_v_y\n",
        "predictions, confidences = predict_naive_bayes(D_test, best_p_y, best_p_v_y)\n",
        "accuracy = len([i for i in range(len(predictions)) if predictions[i] == y_test[i]]) / len(y_test)\n",
        "average_confidence = sum(confidences) / len(confidences)\n",
        "print(\"best k: %g, test accuracy: %.4f, average confidence: %.4f\" %(best_k, accuracy, average_confidence))\n",
        "print(\"CBoW:\")\n",
        "featurizer = CBoWFeaturizer()\n",
        "X_train = convert_to_features(D_train, featurizer, vocab, hashtag_vocab, at_vocab)\n",
        "best_k = 0\n",
        "best_accuracy = 0\n",
        "best_p_y = {}\n",
        "best_p_v_y = {}\n",
        "for k in [0.001, 0.01, 0.1, 1.0, 10.0]:\n",
        "    p_y, p_v_y = train_naive_bayes(X_train, y_train, k, vocab)\n",
        "    predictions, confidences = predict_naive_bayes(D_valid, p_y, p_v_y)\n",
        "    accuracy = len([i for i in range(len(predictions)) if predictions[i] == y_valid[i]]) / len(y_valid)\n",
        "    print(\"k: %g, accuracy: %.4f\" %(k, accuracy))\n",
        "    if accuracy > best_accuracy:\n",
        "        best_k = k\n",
        "        best_accuracy = accuracy\n",
        "        best_p_y = p_y\n",
        "        best_p_v_y = p_v_y\n",
        "predictions, confidences = predict_naive_bayes(D_test, best_p_y, best_p_v_y)\n",
        "accuracy = len([i for i in range(len(predictions)) if predictions[i] == y_test[i]]) / len(y_test)\n",
        "average_confidence = sum(confidences) / len(confidences)\n",
        "print(\"best k: %g, test accuracy: %.4f, average confidence: %.4f\" %(best_k, accuracy, average_confidence))\n",
        "print(\"TFIDF:\")\n",
        "featurizer = TFIDFFeaturizer(compute_idf(D_train, vocab.union(hashtag_vocab).union(at_vocab)))\n",
        "X_train = convert_to_features(D_train, featurizer, vocab, hashtag_vocab, at_vocab)\n",
        "best_k = 0\n",
        "best_accuracy = 0\n",
        "best_p_y = {}\n",
        "best_p_v_y = {}\n",
        "for k in [0.001, 0.01, 0.1, 1.0, 10.0]:\n",
        "    p_y, p_v_y = train_naive_bayes(X_train, y_train, k, vocab)\n",
        "    predictions, confidences = predict_naive_bayes(D_valid, p_y, p_v_y)\n",
        "    accuracy = len([i for i in range(len(predictions)) if predictions[i] == y_valid[i]]) / len(y_valid)\n",
        "    print(\"k: %g, accuracy: %.4f\" %(k, accuracy))\n",
        "    if accuracy > best_accuracy:\n",
        "        best_k = k\n",
        "        best_accuracy = accuracy\n",
        "        best_p_y = p_y\n",
        "        best_p_v_y = p_v_y\n",
        "predictions, confidences = predict_naive_bayes(D_test, best_p_y, best_p_v_y)\n",
        "accuracy = len([i for i in range(len(predictions)) if predictions[i] == y_test[i]]) / len(y_test)\n",
        "average_confidence = sum(confidences) / len(confidences)\n",
        "print(\"best k: %g, test accuracy: %.4f, average confidence: %.4f\" %(best_k, accuracy, average_confidence))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BBoW:\n",
            "k: 0.001, accuracy: 0.3205\n",
            "k: 0.01, accuracy: 0.3207\n",
            "k: 0.1, accuracy: 0.3214\n",
            "k: 1, accuracy: 0.3349\n",
            "k: 10, accuracy: 0.2936\n",
            "best k: 1, test accuracy: 0.3499, average confidence: 0.6686\n",
            "CBoW:\n",
            "k: 0.001, accuracy: 0.3200\n",
            "k: 0.01, accuracy: 0.3205\n",
            "k: 0.1, accuracy: 0.3223\n",
            "k: 1, accuracy: 0.3336\n",
            "k: 10, accuracy: 0.2946\n",
            "best k: 1, test accuracy: 0.3497, average confidence: 0.6671\n",
            "TFIDF:\n",
            "k: 0.001, accuracy: 0.3150\n",
            "k: 0.01, accuracy: 0.3164\n",
            "k: 0.1, accuracy: 0.3191\n",
            "k: 1, accuracy: 0.3237\n",
            "k: 10, accuracy: 0.3313\n",
            "best k: 10, test accuracy: 0.3551, average confidence: 0.7332\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gi5MM49x-Hqb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SECOND EXPERIMENT\n",
        "D_train, y_train = load_dataset('/content/gdrive/Shared drives/CIS519/data/new data/train_20_now.csv')\n",
        "D_valid, y_valid = load_dataset('/content/gdrive/Shared drives/CIS519/data/new data/validation_20_now.csv')\n",
        "\n",
        "f = open('/content/gdrive/Shared drives/CIS519/human_test.txt')\n",
        "\n",
        "D_test = []\n",
        "for line in f:\n",
        "  D_test.append([line.split()])\n",
        "\n",
        "vocab = get_vocabulary([doc[0] for doc in D_train])\n",
        "hashtag_vocab = get_vocabulary([doc[2] for doc in D_train])\n",
        "at_vocab = get_vocabulary([doc[4] for doc in D_train])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmjdBeA8_h-A",
        "colab_type": "code",
        "outputId": "08bba693-af36-4922-b795-569ad1022abd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "print(\"BBoW:\")\n",
        "featurizer = BBoWFeaturizer()\n",
        "X_train = convert_to_features(D_train, featurizer, vocab, hashtag_vocab, at_vocab)\n",
        "best_k = 0\n",
        "best_accuracy = 0\n",
        "best_p_y = {}\n",
        "best_p_v_y = {}\n",
        "for k in [0.001, 0.01, 0.1, 1.0, 10.0]:\n",
        "    p_y, p_v_y = train_naive_bayes(X_train, y_train, k, vocab)\n",
        "    predictions, confidences = predict_naive_bayes(D_valid, p_y, p_v_y)\n",
        "    accuracy = len([i for i in range(len(predictions)) if predictions[i] == y_valid[i]]) / len(y_valid)\n",
        "    print(\"k: %g, accuracy: %.4f\" %(k, accuracy))\n",
        "    if accuracy > best_accuracy:\n",
        "        best_k = k\n",
        "        best_accuracy = accuracy\n",
        "        best_p_y = p_y\n",
        "        best_p_v_y = p_v_y\n",
        "predictions, confidences = predict_naive_bayes(D_test, best_p_y, best_p_v_y)\n",
        "print(predictions)\n",
        "print(\"CBoW:\")\n",
        "featurizer = CBoWFeaturizer()\n",
        "X_train = convert_to_features(D_train, featurizer, vocab, hashtag_vocab, at_vocab)\n",
        "best_k = 0\n",
        "best_accuracy = 0\n",
        "best_p_y = {}\n",
        "best_p_v_y = {}\n",
        "for k in [0.001, 0.01, 0.1, 1.0, 10.0]:\n",
        "    p_y, p_v_y = train_naive_bayes(X_train, y_train, k, vocab)\n",
        "    predictions, confidences = predict_naive_bayes(D_valid, p_y, p_v_y)\n",
        "    accuracy = len([i for i in range(len(predictions)) if predictions[i] == y_valid[i]]) / len(y_valid)\n",
        "    print(\"k: %g, accuracy: %.4f\" %(k, accuracy))\n",
        "    if accuracy > best_accuracy:\n",
        "        best_k = k\n",
        "        best_accuracy = accuracy\n",
        "        best_p_y = p_y\n",
        "        best_p_v_y = p_v_y\n",
        "predictions, confidences = predict_naive_bayes(D_test, best_p_y, best_p_v_y)\n",
        "print(predictions)\n",
        "print(\"TFIDF:\")\n",
        "featurizer = TFIDFFeaturizer(compute_idf(D_train, vocab.union(hashtag_vocab).union(at_vocab)))\n",
        "X_train = convert_to_features(D_train, featurizer, vocab, hashtag_vocab, at_vocab)\n",
        "best_k = 0\n",
        "best_accuracy = 0\n",
        "best_p_y = {}\n",
        "best_p_v_y = {}\n",
        "for k in [0.001, 0.01, 0.1, 1.0, 10.0]:\n",
        "    p_y, p_v_y = train_naive_bayes(X_train, y_train, k, vocab)\n",
        "    predictions, confidences = predict_naive_bayes(D_valid, p_y, p_v_y)\n",
        "    accuracy = len([i for i in range(len(predictions)) if predictions[i] == y_valid[i]]) / len(y_valid)\n",
        "    print(\"k: %g, accuracy: %.4f\" %(k, accuracy))\n",
        "    if accuracy > best_accuracy:\n",
        "        best_k = k\n",
        "        best_accuracy = accuracy\n",
        "        best_p_y = p_y\n",
        "        best_p_v_y = p_v_y\n",
        "predictions, confidences = predict_naive_bayes(D_test, best_p_y, best_p_v_y)\n",
        "print(predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BBoW:\n",
            "k: 0.001, accuracy: 0.3338\n",
            "k: 0.01, accuracy: 0.3377\n",
            "k: 0.1, accuracy: 0.3414\n",
            "k: 1, accuracy: 0.3482\n",
            "k: 10, accuracy: 0.2950\n",
            "[1, 1, 1, 2, 1, 3, 1, 1, 1, 3, 3, 1, 3, 1, 3, 3, 2, 1, 1, 1, 3, 3, 3, 2, 3, 2, 3, 1, 3, 1, 3, 1, 3, 1, 1, 3, 1, 2, 3, 2, 1, 2, 3, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 2, 3, 3, 2, 1, 3, 1, 1, 1, 3, 1, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 1, 3, 3, 3, 1, 3, 1, 1, 3, 3, 9, 2, 3, 4, 3, 3, 1, 3, 1, 3, 1, 3, 1, 3, 7, 2, 3, 1, 1, 3, 3, 1, 1, 1, 3, 1, 1, 2, 1, 3, 3, 19, 3, 1, 3, 3, 3, 1, 3, 1, 4, 2, 3, 3, 3, 2, 3, 1, 1, 1, 4, 4, 3, 3, 1, 3, 3, 1, 3, 3, 1, 1, 2, 3, 6, 1, 3, 3, 3, 1, 1, 1, 3, 3, 3, 3, 3, 3, 1, 1, 2, 3, 1, 3, 4, 1, 1, 2, 3, 1]\n",
            "CBoW:\n",
            "k: 0.001, accuracy: 0.3324\n",
            "k: 0.01, accuracy: 0.3361\n",
            "k: 0.1, accuracy: 0.3395\n",
            "k: 1, accuracy: 0.3503\n",
            "k: 10, accuracy: 0.2950\n",
            "[1, 1, 1, 2, 1, 3, 1, 1, 1, 3, 3, 1, 3, 1, 3, 3, 2, 1, 1, 1, 3, 3, 3, 2, 3, 2, 3, 1, 3, 1, 3, 1, 3, 1, 1, 3, 1, 2, 3, 2, 1, 2, 3, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 2, 6, 3, 2, 1, 3, 1, 1, 1, 3, 1, 3, 3, 3, 3, 4, 3, 1, 1, 3, 3, 3, 3, 3, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 1, 3, 3, 3, 1, 3, 1, 1, 3, 3, 9, 2, 3, 4, 3, 3, 1, 3, 1, 3, 1, 3, 4, 3, 7, 2, 3, 1, 1, 3, 3, 1, 1, 1, 3, 1, 1, 2, 1, 3, 3, 19, 3, 1, 3, 3, 3, 1, 3, 1, 4, 2, 3, 3, 3, 2, 3, 1, 1, 1, 4, 4, 3, 3, 1, 3, 3, 1, 3, 3, 1, 1, 2, 3, 6, 1, 3, 3, 3, 1, 1, 1, 3, 3, 3, 3, 3, 3, 1, 1, 2, 3, 1, 3, 4, 1, 1, 2, 3, 1]\n",
            "TFIDF:\n",
            "k: 0.001, accuracy: 0.3306\n",
            "k: 0.01, accuracy: 0.3310\n",
            "k: 0.1, accuracy: 0.3356\n",
            "k: 1, accuracy: 0.3393\n",
            "k: 10, accuracy: 0.3411\n",
            "[1, 1, 1, 3, 1, 3, 1, 1, 1, 3, 3, 1, 3, 1, 3, 3, 2, 1, 1, 1, 3, 3, 3, 2, 3, 2, 3, 1, 3, 1, 3, 1, 3, 1, 1, 3, 1, 2, 3, 2, 1, 2, 3, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 2, 3, 3, 2, 1, 3, 1, 1, 1, 3, 1, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 1, 3, 3, 3, 1, 3, 1, 1, 3, 3, 1, 2, 3, 1, 3, 3, 1, 3, 1, 3, 1, 3, 1, 3, 3, 2, 3, 1, 1, 3, 3, 1, 1, 1, 3, 1, 1, 3, 1, 3, 3, 3, 3, 1, 3, 3, 3, 1, 3, 1, 1, 2, 3, 3, 3, 2, 3, 1, 1, 1, 4, 3, 3, 3, 1, 3, 3, 1, 3, 3, 1, 1, 2, 3, 6, 1, 3, 3, 3, 1, 1, 1, 3, 3, 3, 3, 3, 3, 1, 1, 2, 3, 1, 3, 3, 1, 1, 2, 3, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}